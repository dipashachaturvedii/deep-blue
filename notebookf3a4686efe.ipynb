{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-17T16:12:34.242029Z","iopub.execute_input":"2023-02-17T16:12:34.243122Z","iopub.status.idle":"2023-02-17T16:12:34.249095Z","shell.execute_reply.started":"2023-02-17T16:12:34.243083Z","shell.execute_reply":"2023-02-17T16:12:34.247988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting up our Data  \nBefore we begin with creating and training our model, we will first set the size of the batches for our training, as well as the image height and width to set for our model","metadata":{}},{"cell_type":"code","source":"batch_size = 100\nimg_height = 250\nimg_width = 250","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:34.262365Z","iopub.execute_input":"2023-02-17T16:12:34.26268Z","iopub.status.idle":"2023-02-17T16:12:34.267538Z","shell.execute_reply.started":"2023-02-17T16:12:34.262652Z","shell.execute_reply":"2023-02-17T16:12:34.266294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset that we are using has 3 different folders, and each of these have 2 folders within them having a folder for accident images and non accident images. Do look and scroll through them to verify and see the structure.  \nIn order to get our:  \n1. train,\n2. test\n3. and validation split,  \n\nwe will use keras's inbuilt *image_dataset_from_directory()* function which is able to generate a tf dataset containing the images as well as their corresponding classes from the folder that we pass into the parameter.","metadata":{}},{"cell_type":"code","source":"training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/train',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size\n\n)\n\ntesting_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/test',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size)\n\nvalidation_ds =  tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/val',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:34.275075Z","iopub.execute_input":"2023-02-17T16:12:34.275353Z","iopub.status.idle":"2023-02-17T16:12:34.831538Z","shell.execute_reply.started":"2023-02-17T16:12:34.275327Z","shell.execute_reply":"2023-02-17T16:12:34.830595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice the output reading the files as well as the classes it recognises!  \nNow, we'll set up a few performace parameters that will enhance runtime training of our model. I've learnt to use this from [this excellent notebook here](https://www.kaggle.com/code/vanvalkenberg/cnn-for-accident-detection-83-val-accuracy/notebook), so do check that out as well!","metadata":{}},{"cell_type":"code","source":"class_names = training_ds.class_names\n\n## Configuring dataset for performance\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ntraining_ds = training_ds.cache().prefetch(buffer_size=AUTOTUNE)\ntesting_ds = testing_ds.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:34.833336Z","iopub.execute_input":"2023-02-17T16:12:34.833675Z","iopub.status.idle":"2023-02-17T16:12:34.842405Z","shell.execute_reply.started":"2023-02-17T16:12:34.83364Z","shell.execute_reply":"2023-02-17T16:12:34.841224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining our Pre-Trained Model  \nThe next step is defining and creating our model. In order to increase accuracy and speed up training process, we'll go ahead and use a pre trained model for this task. Why you may ask?  This is because a pretrained convnet already has a very good idea of what features to look for in an image and can find them very effectively since it hs been trained on millions of images. So, if we can determine the presence of features all the rest of the model needs to do is determine which combination of features makes a specific image.  \nSo all we've to do is:\n1. Define the base pretrained layer\n2. Add final few layers that are specific to our function and task to enhance ability in those categories\n3. Train our model!  \nLets use Googles MobileNetV2 for this purpose...\n","metadata":{}},{"cell_type":"code","source":"img_shape = (img_height, img_width, 3)\n\nbase_model = tf.keras.applications.MobileNetV2(input_shape=img_shape,\n                                               include_top=False,\n                                               weights='imagenet')\n\nbase_model.trainable = False","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:34.843823Z","iopub.execute_input":"2023-02-17T16:12:34.844388Z","iopub.status.idle":"2023-02-17T16:12:35.882211Z","shell.execute_reply.started":"2023-02-17T16:12:34.844352Z","shell.execute_reply":"2023-02-17T16:12:35.880968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice how we set trainable to false in order to make sure model wonâ€™t make any changes to the weights of any layers that are already frozen during training.  \nWe also exclude the top of the model since we will perform classification on our own.","metadata":{}},{"cell_type":"markdown","source":"# Creating Final Model  \nWe now go ahead and create our final model which consists of the base model, and 3 more layers for performing convolution. The 2d output of the convolution layer is flattened and fed to a dense output layer to perform the classification. ","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    base_model,\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.Conv2D(128, 3, activation='relu'),\n    layers.Flatten(),\n    layers.Dense(len(class_names), activation= 'softmax')\n])","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:35.885711Z","iopub.execute_input":"2023-02-17T16:12:35.886187Z","iopub.status.idle":"2023-02-17T16:12:36.36432Z","shell.execute_reply.started":"2023-02-17T16:12:35.886142Z","shell.execute_reply":"2023-02-17T16:12:36.363316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:36.367816Z","iopub.execute_input":"2023-02-17T16:12:36.368146Z","iopub.status.idle":"2023-02-17T16:12:36.381824Z","shell.execute_reply.started":"2023-02-17T16:12:36.368118Z","shell.execute_reply":"2023-02-17T16:12:36.380766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll let our model run for 50 epochs, which seems like a decent enough number. Increasing the epochs should result in an increase in accuracy uptil a certain point only though...","metadata":{}},{"cell_type":"code","source":"history = model.fit(training_ds, validation_data = validation_ds, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:36.384051Z","iopub.execute_input":"2023-02-17T16:12:36.384846Z","iopub.status.idle":"2023-02-17T16:12:52.15929Z","shell.execute_reply.started":"2023-02-17T16:12:36.384792Z","shell.execute_reply":"2023-02-17T16:12:52.15835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label = 'training loss')\nplt.plot(history.history['accuracy'], label = 'training accuracy')\nplt.grid(True)\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:52.162795Z","iopub.execute_input":"2023-02-17T16:12:52.163109Z","iopub.status.idle":"2023-02-17T16:12:52.495378Z","shell.execute_reply.started":"2023-02-17T16:12:52.163083Z","shell.execute_reply":"2023-02-17T16:12:52.49444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['val_loss'], label = 'validation loss')\nplt.plot(history.history['val_accuracy'], label = 'validation accuracy')\nplt.grid(True)\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:52.499548Z","iopub.execute_input":"2023-02-17T16:12:52.500186Z","iopub.status.idle":"2023-02-17T16:12:52.796782Z","shell.execute_reply.started":"2023-02-17T16:12:52.500143Z","shell.execute_reply":"2023-02-17T16:12:52.795852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function below looks a bit complicated, but is a simple helper function which shows the image, the predicted class and the actual class for each image in the test dataset. Run it and have a look at how accurate the model seems and where it seems to be struggling.","metadata":{}},{"cell_type":"code","source":"AccuracyVector = []\nplt.figure(figsize=(30, 30))\nfor images, labels in testing_ds.take(1):\n    predictions = model.predict(images)\n    predlabel = []\n    prdlbl = []\n    \n    for mem in predictions:\n        predlabel.append(class_names[np.argmax(mem)])\n        prdlbl.append(np.argmax(mem))\n    \n    AccuracyVector = np.array(prdlbl) == labels\n    for i in range(40):\n        ax = plt.subplot(10, 4, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title('Pred: '+ predlabel[i]+' actl:'+class_names[labels[i]] )\n        plt.axis('off')\n        plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:52.798396Z","iopub.execute_input":"2023-02-17T16:12:52.798679Z","iopub.status.idle":"2023-02-17T16:12:58.125464Z","shell.execute_reply.started":"2023-02-17T16:12:52.798652Z","shell.execute_reply":"2023-02-17T16:12:58.119012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can go ahead and view the models layers through the plot_model function below provided by keras for an intuitive view.","metadata":{}},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:58.128862Z","iopub.execute_input":"2023-02-17T16:12:58.129794Z","iopub.status.idle":"2023-02-17T16:12:58.556831Z","shell.execute_reply.started":"2023-02-17T16:12:58.129756Z","shell.execute_reply":"2023-02-17T16:12:58.555642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And thats all! We've successfully creating a model with an accuracy of around 90%. Notice that this can be further improved by performing image manipulation, performing pooling and training our model for a longer epoch or even adding more layers.. However, for our use case, this model we created is perfectly fine.  ","metadata":{}},{"cell_type":"code","source":"print(class_names)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:58.558729Z","iopub.execute_input":"2023-02-17T16:12:58.561244Z","iopub.status.idle":"2023-02-17T16:12:58.569044Z","shell.execute_reply.started":"2023-02-17T16:12:58.561194Z","shell.execute_reply":"2023-02-17T16:12:58.567967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Model on Videos  \nIn order to use our model on a video, which is our expected use case of a CCTV footage, we will have to use OpenCV in order get the individual frames.  \nLets define a function which takes in each frame and converts it into a tensor and then predicts the output class.","metadata":{}},{"cell_type":"code","source":"def predict_frame(img):\n    img_array = tf.keras.utils.img_to_array(img)\n    img_batch = np.expand_dims(img_array, axis=0)\n    prediction=(model.predict(img_batch) > 0.5).astype(\"int32\")\n    if(prediction[0][0]==0):\n        return(\"Accident Detected\")\n    else:\n        return(\"No Accident\")","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:58.571943Z","iopub.execute_input":"2023-02-17T16:12:58.57224Z","iopub.status.idle":"2023-02-17T16:12:58.579949Z","shell.execute_reply.started":"2023-02-17T16:12:58.572214Z","shell.execute_reply":"2023-02-17T16:12:58.578972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code below makes use of OpenCV. Firstly, we read the video in and grab every 20th frame(in order to reduce total computation for this demonstration) and then we can resize the image and run our function on it.  \nWe'll store the label and the image in a list which we can easily access.","metadata":{}},{"cell_type":"code","source":"import cv2\nimage=[]\nlabel=[]\n\nc=1\ncap= cv2.VideoCapture('/kaggle/input/cctvfootagevideo/videoplayback (online-video-cutter.com).mp4')\nwhile True:\n    grabbed, frame = cap.read()\n    if c%30==0:\n        print(c)\n        resized_frame=tf.keras.preprocessing.image.smart_resize(frame, (img_height, img_width), interpolation='bilinear')\n        image.append(frame)\n        label.append(predict_frame(resized_frame))\n        if(len(image)==75):\n            break\n    c+=1\n\ncap.release()","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:12:58.581784Z","iopub.execute_input":"2023-02-17T16:12:58.582352Z","iopub.status.idle":"2023-02-17T16:13:06.187815Z","shell.execute_reply.started":"2023-02-17T16:12:58.582307Z","shell.execute_reply":"2023-02-17T16:13:06.18681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see any random frame and see what the outcome is...","metadata":{}},{"cell_type":"code","source":"print(label[10])\nprint(plt.imshow(image[10]))","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:13:06.189273Z","iopub.execute_input":"2023-02-17T16:13:06.190114Z","iopub.status.idle":"2023-02-17T16:13:06.560441Z","shell.execute_reply.started":"2023-02-17T16:13:06.190076Z","shell.execute_reply":"2023-02-17T16:13:06.559497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks about right! There seems to be an accident occuring in this frame. Our model generalizes well and can be used for practical applications.","metadata":{}},{"cell_type":"markdown","source":"# Converting to TFLite Model  \nWhile we've made our model, it is true that Tensor Flow models are very large and bulky and not suitable for the small processing powers that a CCTV surveillance system will handle. For this purpose, we'll convert our Tf model into a TFLite model through the API's available by keras.","metadata":{}},{"cell_type":"code","source":"# Convert the model.\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('tf_lite_model.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:13:06.563394Z","iopub.execute_input":"2023-02-17T16:13:06.564121Z","iopub.status.idle":"2023-02-17T16:13:41.344643Z","shell.execute_reply.started":"2023-02-17T16:13:06.564079Z","shell.execute_reply":"2023-02-17T16:13:41.343603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A TFLite model is referred to as an interpreter. We open it up and have a look at the input and output shape. It should be a single image of height and width 250 by 250 with 3 colour channels.  \nThe output can be of 2 types only. Accident or Non Accident.","metadata":{}},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(model_path = 'tf_lite_model.tflite')\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nprint(\"Input Shape:\", input_details[0]['shape'])\nprint(\"Input Type:\", input_details[0]['dtype'])\nprint(\"Output Shape:\", output_details[0]['shape'])\nprint(\"Output Type:\", output_details[0]['dtype'])","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:13:41.346328Z","iopub.execute_input":"2023-02-17T16:13:41.346688Z","iopub.status.idle":"2023-02-17T16:13:41.357256Z","shell.execute_reply.started":"2023-02-17T16:13:41.346653Z","shell.execute_reply":"2023-02-17T16:13:41.356104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While the steps below aren't necessary, I'll still show you incase you have to perform a similair task for a different model where the input tensor might change or be different.","metadata":{}},{"cell_type":"code","source":"interpreter.resize_tensor_input(input_details[0]['index'], (1, 250, 250,3))\ninterpreter.resize_tensor_input(output_details[0]['index'], (1, 2))\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nprint(\"Input Shape:\", input_details[0]['shape'])\nprint(\"Input Type:\", input_details[0]['dtype'])\nprint(\"Output Shape:\", output_details[0]['shape'])\nprint(\"Output Type:\", output_details[0]['dtype'])","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:13:41.358739Z","iopub.execute_input":"2023-02-17T16:13:41.359343Z","iopub.status.idle":"2023-02-17T16:13:41.386124Z","shell.execute_reply.started":"2023-02-17T16:13:41.359297Z","shell.execute_reply":"2023-02-17T16:13:41.385304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying Our TFLite Model Out  \nWe'll try our TFLite model on a random image and see what our output is and if it works. ","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nim=Image.open(\"/kaggle/input/accident-detection-from-cctv-footage/data/train/Non Accident/5_17.jpg\").resize((250,250))\nimg_array = tf.keras.utils.img_to_array(im)\nimg_batch = np.expand_dims(img_array, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:13:41.388906Z","iopub.execute_input":"2023-02-17T16:13:41.389688Z","iopub.status.idle":"2023-02-17T16:13:41.411615Z","shell.execute_reply.started":"2023-02-17T16:13:41.389652Z","shell.execute_reply":"2023-02-17T16:13:41.410899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below lines are equivalent to performing a prediction in a TF model. *interpretor.get_tensor(*) performs the prediction.","metadata":{}},{"cell_type":"code","source":"interpreter.set_tensor(input_details[0]['index'], img_batch)\ninterpreter.invoke()\ntflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\nprint(\"Prediction results:\", tflite_model_predictions)\nprint(plt.imshow(im))","metadata":{"execution":{"iopub.status.busy":"2023-02-17T16:13:41.412824Z","iopub.execute_input":"2023-02-17T16:13:41.413817Z","iopub.status.idle":"2023-02-17T16:13:41.725816Z","shell.execute_reply.started":"2023-02-17T16:13:41.41378Z","shell.execute_reply":"2023-02-17T16:13:41.724817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It works. We've got a complete end to end system for accident detection now that should work very well indeed.","metadata":{}}]}